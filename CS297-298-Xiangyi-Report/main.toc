\contentsline {section}{\numberline {1}Introduction}{1}{}%
\contentsline {section}{\numberline {2}Related Work}{2}{}%
\contentsline {subsection}{\numberline {2.1}Dimensional vs. Categorical Emotion Models}{2}{}%
\contentsline {subsection}{\numberline {2.2}Early Emotion-Recognition Approaches (pre-2012)}{3}{}%
\contentsline {subsection}{\numberline {2.3}Deep-Learning Era (2013–2017)}{4}{}%
\contentsline {subsection}{\numberline {2.4}Transformer-Based Models (2018–2025)}{4}{}%
\contentsline {subsection}{\numberline {2.5}Multimodal Fusion Taxonomy}{4}{}%
\contentsline {subsection}{\numberline {2.6}Benchmark Datasets}{5}{}%
\contentsline {subsection}{\numberline {2.7}Current Challenges}{5}{}%
\contentsline {subsection}{\numberline {2.8}Audio-Based Emotion Detection}{7}{}%
\contentsline {subsection}{\numberline {2.9}Multimodal Approaches}{7}{}%
\contentsline {subsection}{\numberline {2.10}Emotion Recognition Datasets}{8}{}%
\contentsline {section}{\numberline {3}Methodology}{8}{}%
\contentsline {subsection}{\numberline {3.1}System Architecture Overview}{9}{}%
\contentsline {subsection}{\numberline {3.2}Text Processing Models}{10}{}%
\contentsline {subsubsection}{\numberline {3.2.1}BERT (Bidirectional Encoder Representations from Transformers)}{10}{}%
\contentsline {subsubsection}{\numberline {3.2.2}RoBERTa (Robustly Optimized BERT Approach)}{11}{}%
\contentsline {subsubsection}{\numberline {3.2.3}XLNet}{11}{}%
\contentsline {subsubsection}{\numberline {3.2.4}ALBERT (A Lite BERT)}{12}{}%
\contentsline {subsubsection}{\numberline {3.2.5}ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)}{12}{}%
\contentsline {subsubsection}{\numberline {3.2.6}DeBERTa (Decoding-enhanced BERT with disentangled attention)}{13}{}%
\contentsline {subsection}{\numberline {3.3}Text Model Training Procedure}{13}{}%
\contentsline {paragraph}{Preprocessing:}{13}{}%
\contentsline {paragraph}{Hyperparameters:}{13}{}%
\contentsline {paragraph}{Regularization Techniques:}{14}{}%
\contentsline {paragraph}{Loss Function:}{14}{}%
\contentsline {subsection}{\numberline {3.4}Audio Feature Extraction}{14}{}%
\contentsline {subsubsection}{\numberline {3.4.1}Mel-Frequency Cepstral Coefficients (MFCCs)}{14}{}%
\contentsline {subsubsection}{\numberline {3.4.2}Spectrograms}{15}{}%
\contentsline {subsubsection}{\numberline {3.4.3}Prosodic Features}{16}{}%
\contentsline {subsubsection}{\numberline {3.4.4}Wav2vec Embeddings}{16}{}%
\contentsline {subsection}{\numberline {3.5}Audio Processing Models}{16}{}%
\contentsline {subsubsection}{\numberline {3.5.1}CNN for Spectrograms and MFCCs}{17}{}%
\contentsline {subsubsection}{\numberline {3.5.2}BiLSTM for Prosodic Features and Wav2vec Embeddings}{17}{}%
\contentsline {subsection}{\numberline {3.6}Fusion Strategies}{18}{}%
\contentsline {subsubsection}{\numberline {3.6.1}Early Fusion}{19}{}%
\contentsline {subsubsection}{\numberline {3.6.2}Late Fusion}{19}{}%
\contentsline {subsubsection}{\numberline {3.6.3}Hybrid Fusion}{19}{}%
\contentsline {subsubsection}{\numberline {3.6.4}Attention-Based Fusion}{20}{}%
\contentsline {subsection}{\numberline {3.7}Implementation Framework}{22}{}%
\contentsline {subsection}{\numberline {3.8}Training Protocol}{22}{}%
\contentsline {subsection}{\numberline {3.9}Evaluation Metrics}{23}{}%
\contentsline {subsection}{\numberline {3.10}Cross-Validation Strategy}{25}{}%
\contentsline {subsection}{\numberline {3.11}Experimental Configurations}{25}{}%
\contentsline {section}{\numberline {4}Results}{26}{}%
\contentsline {subsection}{\numberline {4.1}Dimensional Emotion Prediction (Stage 1)}{26}{}%
\contentsline {subsection}{\numberline {4.2}Mapping to Categorical Emotions (Stage 2)}{27}{}%
\contentsline {section}{\numberline {5}Discussion}{28}{}%
\contentsline {subsection}{\numberline {5.1}Two-Stage Approach vs. Direct Classification}{28}{}%
\contentsline {subsection}{\numberline {5.2}Model Selection for Emotion Detection}{29}{}%
\contentsline {subsubsection}{\numberline {5.2.1}Transformer Model Performance Analysis}{29}{}%
\contentsline {paragraph}{Understanding RoBERTa's Advantage:}{30}{}%
\contentsline {paragraph}{Model Selection Considerations:}{30}{}%
\contentsline {paragraph}{Comparison with Prior Work:}{31}{}%
\contentsline {subsection}{\numberline {5.3}Modality Importance}{32}{}%
\contentsline {subsubsection}{\numberline {5.3.1}Text vs. Audio Modalities}{32}{}%
\contentsline {paragraph}{Interpreting Unimodal Performance:}{33}{}%
\contentsline {paragraph}{Modality Contributions:}{33}{}%
\contentsline {subsection}{\numberline {5.4}Audio Feature Effectiveness}{34}{}%
\contentsline {subsubsection}{\numberline {5.4.1}Comparative Analysis of Audio Representations}{34}{}%
\contentsline {subsection}{\numberline {5.5}Fusion Strategy Considerations}{35}{}%
\contentsline {subsubsection}{\numberline {5.5.1}Comparative Effectiveness of Fusion Approaches}{35}{}%
\contentsline {paragraph}{Feature-Specific Fusion Patterns:}{35}{}%
\contentsline {paragraph}{Attention Mechanism Challenges:}{36}{}%
\contentsline {subsection}{\numberline {5.6}Dataset Considerations}{36}{}%
\contentsline {subsubsection}{\numberline {5.6.1}Impact of Dataset Selection}{36}{}%
\contentsline {subsection}{\numberline {5.7}Practical Implications}{37}{}%
\contentsline {subsubsection}{\numberline {5.7.1}Model Selection Guidelines}{37}{}%
\contentsline {subsection}{\numberline {5.8}Comparison with State-of-the-Art}{38}{}%
\contentsline {subsubsection}{\numberline {5.8.1}Benchmarking Against Existing Approaches}{38}{}%
\contentsline {subsection}{\numberline {5.9}Limitations}{39}{}%
\contentsline {subsubsection}{\numberline {5.9.1}Technical Limitations}{39}{}%
\contentsline {paragraph}{Methodological Limitations:}{39}{}%
\contentsline {subsection}{\numberline {5.10}Future Directions}{40}{}%
\contentsline {subsection}{\numberline {5.11}Ethical Considerations}{41}{}%
\contentsline {subsection}{\numberline {5.12}Theoretical Implications and Novel Insights}{42}{}%
\contentsline {subsection}{\numberline {5.13}Critical Limitations and Research Opportunities}{42}{}%
\contentsline {section}{\numberline {6}Conclusion and Future Work}{43}{}%
\contentsline {subsection}{\numberline {6.1}Summary of Findings}{43}{}%
\contentsline {subsection}{\numberline {6.2}Theoretical and Practical Contributions}{44}{}%
\contentsline {paragraph}{Theoretical Contributions:}{44}{}%
\contentsline {paragraph}{Practical Contributions:}{45}{}%
\contentsline {paragraph}{Methodological Contributions:}{45}{}%
\contentsline {subsection}{\numberline {6.3}Limitations}{46}{}%
\contentsline {paragraph}{Dataset Limitations:}{46}{}%
\contentsline {paragraph}{Technical Limitations:}{46}{}%
\contentsline {paragraph}{Evaluation Limitations:}{47}{}%
\contentsline {subsection}{\numberline {6.4}Future Directions}{47}{}%
\contentsline {paragraph}{Technical Improvements:}{47}{}%
\contentsline {paragraph}{Architectural Innovations:}{48}{}%
\contentsline {paragraph}{Dataset and Evaluation Extensions:}{48}{}%
\contentsline {paragraph}{Real-World Deployment Challenges:}{49}{}%
\contentsline {paragraph}{Application Domains:}{49}{}%
\contentsline {paragraph}{Responsible Development:}{50}{}%
\contentsline {subsection}{\numberline {6.5}Final Thoughts}{50}{}%
\contentsline {subsection}{\numberline {6.6}Critical Limitations and Research Opportunities}{51}{}%
\contentsline {subsection}{\numberline {6.7}Critical Analysis of Feature-Fusion Interactions}{52}{}%
\contentsline {subsection}{\numberline {6.8}Ablation Studies and Component Analysis}{53}{}%
\contentsline {subsection}{\numberline {6.9}Analysis of Emotion Misclassifications}{53}{}%
