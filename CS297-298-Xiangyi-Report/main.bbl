% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{mao2014learning}
Q.~Mao, M.~Dong, Z.~Huang, and Y.~Zhan, ``Learning salient features for speech
  emotion recognition using convolutional neural networks,'' in \emph{IEEE
  transactions on multimedia}, vol.~16, no.~8.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2014, pp. 2203--2213.

\bibitem{abdul2017emonet}
M.~Abdul-Mageed and L.~Ungar, ``Emonet: Fine-grained emotion detection with
  gated recurrent neural networks,'' \emph{Proceedings of the 55th annual
  meeting of the association for computational linguistics}, vol.~1, pp.
  718--728, 2017.

\bibitem{poria2018multimodal}
S.~Poria, E.~Cambria, and et~al., ``Multimodal sentiment analysis: Addressing
  key issues and setting up the baselines,'' \emph{IEEE Intelligent Systems},
  2018.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: A robustly optimized bert
  pretraining approach,'' 2019.

\bibitem{tsai2019mult}
Y.-H.~H. Tsai and et~al., ``Mult: Multimodal transformer for emotion
  recognition,'' in \emph{ACL}, 2019.

\bibitem{lv2021progressive}
J.~Lv and et~al., ``Progressive modality reinforcement for multimodal emotion
  recognition,'' in \emph{ICASSP}, 2021.

\bibitem{wang2020context}
T.~Wang and et~al., ``Context-aware multimodal emotion recognition via a new
  unified transformer framework,'' in \emph{Proceedings of ACM MM}, 2020.

\bibitem{siriwardhana2020joint}
C.~Siriwardhana and et~al., ``Jointly fine-tuning bert-based representations
  for multimodal emotion recognition,'' in \emph{Proceedings of ICASSP}, 2020.

\bibitem{poria2017review}
S.~Poria, E.~Cambria, R.~Bajpai, and A.~Hussain, ``A review of affective
  computing: From unimodal analysis to multimodal fusion,'' \emph{Information
  Fusion}, vol.~37, pp. 98--125, 2017.

\bibitem{wang2019words}
Y.~Wang, A.~Zadeh, and L.~Morency, ``Words can shift: Dynamically adjusting
  word representations using nonverbal behaviors,'' in \emph{Proceedings of
  ACL}, 2019.

\bibitem{zadeh2018multimodal_tfn}
A.~e.~a. Zadeh, ``Tensor fusion network for multimodal sentiment analysis,'' in
  \emph{EMNLP}, 2017.

\bibitem{zadeh2018mfn}
------, ``Memory fusion network for multi-view sequential learning,'' in
  \emph{AAAI}, 2018.

\bibitem{mittal2020m3er}
T.~Mittal, U.~Bhattacharya, and et~al., ``M3er: Multiplicative multimodal
  emotion recognition using facial, textual, and speech cues,'' in \emph{AAAI},
  2020.

\bibitem{busso2008iemocap}
C.~Busso, M.~Bulut, C.-C. Lee, A.~Kazemzadeh, E.~Mower, S.~Kim, J.~N. Chang,
  S.~Lee, and S.~S. Narayanan, ``Iemocap: Interactive emotional dyadic motion
  capture database,'' \emph{Language resources and evaluation}, vol.~42, no.~4,
  pp. 335--359, 2008.

\bibitem{zadeh2016mosi}
A.~e.~a. Zadeh, ``Multimodal sentiment intensity analysis in videos,'' in
  \emph{ACL}, 2016.

\bibitem{zadeh2018multimodal}
A.~Zadeh, P.~P. Liang, S.~Poria, E.~Cambria, and L.-P. Morency, ``Multimodal
  language analysis in the wild: Cmu-mosei dataset and interpretable dynamic
  fusion graph,'' in \emph{Proceedings of the 56th Annual Meeting of the
  Association for Computational Linguistics}, vol.~1, 2018, pp. 2236--2246.

\bibitem{poria2018meld}
S.~e.~a. Poria, ``Meld: A multimodal multi-party dataset for emotion
  recognition in conversations,'' in \emph{ACL}, 2018.

\bibitem{livingstone2018ravdess}
Unknown, ``Missing bibliography entry for livingstone2018ravdess,'' 2023, this
  is an automatically generated placeholder.

\bibitem{schuller2009acoustic}
B.~Schuller, S.~Steidl, and A.~Batliner, ``Acoustic emotion recognition: A
  benchmark comparison of performances,'' in \emph{2009 IEEE Workshop on
  Automatic Speech Recognition \& Understanding}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2009, pp. 552--557.

\bibitem{li2013speech}
L.~Li, Y.~Zhao, D.~Jiang, and Y.~Zhang, ``Speech emotion recognition using
  hidden markov models,'' \emph{Mobile Multimedia Processing: Fundamentals,
  Methods, and Applications}, pp. 244--254, 2013.

\bibitem{schneider2019wav2vec}
S.~Schneider, A.~Baevski, R.~Collobert, and M.~Auli, ``wav2vec: Unsupervised
  pre-training for speech recognition,'' \emph{arXiv preprint
  arXiv:1904.05862}, 2019.

\bibitem{wagner2011introducting}
J.~Wagner, F.~Lingenfelser, T.~Baur, I.~Damian, F.~Kistler, and E.~Andr√©,
  ``Introducing currennt: The munich open-source cuda recurrent neural network
  toolkit,'' \emph{The Journal of Machine Learning Research}, vol.~12, pp.
  2633--2637, 2011.

\bibitem{zadeh2018memory}
A.~Zadeh, P.~P. Liang, N.~Mazumder, S.~Poria, E.~Cambria, and L.-P. Morency,
  ``Memory fusion network for multi-view sequential learning,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~32, no.~1, 2018.

\bibitem{kiela2019supervised}
D.~Kiela, S.~Bhooshan, H.~Firooz, and D.~Davison, ``Supervised multimodal
  bitransformers for classifying images and text,'' in \emph{arXiv preprint
  arXiv:1909.02950}, 2019.

\bibitem{mckeown2012semaine}
G.~McKeown, M.~Valstar, R.~Cowie, M.~Pantic, and M.~Schroder, ``The semaine
  database: Annotated multimodal records of emotionally colored conversations
  between a person and a limited agent,'' \emph{IEEE transactions on affective
  computing}, vol.~3, no.~1, pp. 5--17, 2012.

\bibitem{livingstone2018ryerson}
S.~R. Livingstone and F.~A. Russo, ``The ryerson audio-visual database of
  emotional speech and song (ravdess): A dynamic, multimodal set of facial and
  vocal expressions in north american english,'' \emph{PloS one}, vol.~13,
  no.~5, p. e0196391, 2018.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{yang2019xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~Salakhutdinov, and Q.~V. Le,
  ``Xlnet: Generalized autoregressive pretraining for language understanding,''
  \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{lan2019albert}
Z.~Lan, M.~Chen, S.~Goodman, K.~Gimpel, P.~Sharma, and R.~Soricut, ``Albert: A
  lite bert for self-supervised learning of language representations,''
  \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem{clark2020electra}
K.~Clark, M.-T. Luong, Q.~V. Le, and C.~D. Manning, ``Electra: Pre-training
  text encoders as discriminators rather than generators,'' \emph{arXiv
  preprint arXiv:2003.10555}, 2020.

\bibitem{he2020deberta}
P.~He, X.~Liu, J.~Gao, and W.~Chen, ``Deberta: Decoding-enhanced bert with
  disentangled attention,'' \emph{arXiv preprint arXiv:2006.03654}, 2020.

\bibitem{sehrawat2023deception}
P.~K. Sehrawat, R.~Kumar, N.~Kumar, and D.~K. Vishwakarma, ``Deception
  detection using a multimodal stacked bi-lstm model,'' in \emph{2023
  International Conference on Innovative Data Communication Technologies and
  Application (ICIDCA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp.
  318--326.

\bibitem{hsiao2022attention}
S.-W. Hsiao and C.-Y. Sun, ``Attention-aware multi-modal rnn for deception
  detection,'' in \emph{2022 IEEE International Conference on Big Data (Big
  Data)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 3593--3596.

\bibitem{zhang2022fine}
H.~Zhang, Y.~Ding, L.~Cao, X.~Wang, and L.~Feng, ``Fine-grained question-level
  deception detection via graph-based learning and cross-modal fusion,''
  \emph{IEEE Transactions on Information Forensics and Security}, vol.~17, pp.
  2452--2467, 2022.

\end{thebibliography}
