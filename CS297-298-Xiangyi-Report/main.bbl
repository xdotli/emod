% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{mohammad2013crowdsourcing}
S.~M. Mohammad and P.~D. Turney, ``Crowdsourcing a word-emotion association
  lexicon,'' \emph{Computational Intelligence}, vol.~29, no.~3, pp. 436--465,
  2013.

\bibitem{wang2012harnessing}
W.~Wang, L.~Chen, K.~Thirunarayan, and A.~P. Sheth, ``Harnessing wikipedia for
  automatic emotion detection,'' in \emph{2012 IEEE 12th International
  Conference on Data Mining Workshops}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2012, pp. 799--806.

\bibitem{abdul2017emonet}
M.~Abdul-Mageed and L.~Ungar, ``Emonet: Fine-grained emotion detection with
  gated recurrent neural networks,'' \emph{Proceedings of the 55th annual
  meeting of the association for computational linguistics}, vol.~1, pp.
  718--728, 2017.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: A robustly optimized bert
  pretraining approach,'' 2019.

\bibitem{yang2019xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~Salakhutdinov, and Q.~V. Le,
  ``Xlnet: Generalized autoregressive pretraining for language understanding,''
  \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{clark2020electra}
K.~Clark, M.-T. Luong, Q.~V. Le, and C.~D. Manning, ``Electra: Pre-training
  text encoders as discriminators rather than generators,'' \emph{arXiv
  preprint arXiv:2003.10555}, 2020.

\bibitem{schuller2009acoustic}
B.~Schuller, S.~Steidl, and A.~Batliner, ``Acoustic emotion recognition: A
  benchmark comparison of performances,'' in \emph{2009 IEEE Workshop on
  Automatic Speech Recognition \& Understanding}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2009, pp. 552--557.

\bibitem{li2013speech}
L.~Li, Y.~Zhao, D.~Jiang, and Y.~Zhang, ``Speech emotion recognition using
  hidden markov models,'' \emph{Mobile Multimedia Processing: Fundamentals,
  Methods, and Applications}, pp. 244--254, 2013.

\bibitem{mao2014learning}
Q.~Mao, M.~Dong, Z.~Huang, and Y.~Zhan, ``Learning salient features for speech
  emotion recognition using convolutional neural networks,'' in \emph{IEEE
  transactions on multimedia}, vol.~16, no.~8.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2014, pp. 2203--2213.

\bibitem{schneider2019wav2vec}
S.~Schneider, A.~Baevski, R.~Collobert, and M.~Auli, ``wav2vec: Unsupervised
  pre-training for speech recognition,'' \emph{arXiv preprint
  arXiv:1904.05862}, 2019.

\bibitem{poria2017review}
S.~Poria, E.~Cambria, R.~Bajpai, and A.~Hussain, ``A review of affective
  computing: From unimodal analysis to multimodal fusion,'' \emph{Information
  Fusion}, vol.~37, pp. 98--125, 2017.

\bibitem{wagner2011introducting}
J.~Wagner, F.~Lingenfelser, T.~Baur, I.~Damian, F.~Kistler, and E.~Andr√©,
  ``Introducing currennt: The munich open-source cuda recurrent neural network
  toolkit,'' \emph{The Journal of Machine Learning Research}, vol.~12, pp.
  2633--2637, 2011.

\bibitem{zadeh2018memory}
A.~Zadeh, P.~P. Liang, N.~Mazumder, S.~Poria, E.~Cambria, and L.-P. Morency,
  ``Memory fusion network for multi-view sequential learning,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~32, no.~1, 2018.

\bibitem{kiela2019supervised}
D.~Kiela, S.~Bhooshan, H.~Firooz, and D.~Davison, ``Supervised multimodal
  bitransformers for classifying images and text,'' in \emph{arXiv preprint
  arXiv:1909.02950}, 2019.

\bibitem{busso2008iemocap}
C.~Busso, M.~Bulut, C.-C. Lee, A.~Kazemzadeh, E.~Mower, S.~Kim, J.~N. Chang,
  S.~Lee, and S.~S. Narayanan, ``Iemocap: Interactive emotional dyadic motion
  capture database,'' \emph{Language resources and evaluation}, vol.~42, no.~4,
  pp. 335--359, 2008.

\bibitem{mckeown2012semaine}
G.~McKeown, M.~Valstar, R.~Cowie, M.~Pantic, and M.~Schroder, ``The semaine
  database: Annotated multimodal records of emotionally colored conversations
  between a person and a limited agent,'' \emph{IEEE transactions on affective
  computing}, vol.~3, no.~1, pp. 5--17, 2012.

\bibitem{livingstone2018ryerson}
S.~R. Livingstone and F.~A. Russo, ``The ryerson audio-visual database of
  emotional speech and song (ravdess): A dynamic, multimodal set of facial and
  vocal expressions in north american english,'' \emph{PloS one}, vol.~13,
  no.~5, p. e0196391, 2018.

\bibitem{zadeh2018multimodal}
A.~Zadeh, P.~P. Liang, S.~Poria, E.~Cambria, and L.-P. Morency, ``Multimodal
  language analysis in the wild: Cmu-mosei dataset and interpretable dynamic
  fusion graph,'' in \emph{Proceedings of the 56th Annual Meeting of the
  Association for Computational Linguistics}, vol.~1, 2018, pp. 2236--2246.

\bibitem{lan2019albert}
Z.~Lan, M.~Chen, S.~Goodman, K.~Gimpel, P.~Sharma, and R.~Soricut, ``Albert: A
  lite bert for self-supervised learning of language representations,''
  \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem{he2020deberta}
P.~He, X.~Liu, J.~Gao, and W.~Chen, ``Deberta: Decoding-enhanced bert with
  disentangled attention,'' \emph{arXiv preprint arXiv:2006.03654}, 2020.

\bibitem{sehrawat2023deception}
P.~K. Sehrawat, R.~Kumar, N.~Kumar, and D.~K. Vishwakarma, ``Deception
  detection using a multimodal stacked bi-lstm model,'' in \emph{2023
  International Conference on Innovative Data Communication Technologies and
  Application (ICIDCA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp.
  318--326.

\bibitem{hsiao2022attention}
S.-W. Hsiao and C.-Y. Sun, ``Attention-aware multi-modal rnn for deception
  detection,'' in \emph{2022 IEEE International Conference on Big Data (Big
  Data)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 3593--3596.

\bibitem{zhang2022fine}
H.~Zhang, Y.~Ding, L.~Cao, X.~Wang, and L.~Feng, ``Fine-grained question-level
  deception detection via graph-based learning and cross-modal fusion,''
  \emph{IEEE Transactions on Information Forensics and Security}, vol.~17, pp.
  2452--2467, 2022.

\end{thebibliography}
